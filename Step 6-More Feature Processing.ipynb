{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was loaded with:\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./dse/bin/dse pyspark --num-executors 5 --driver-memory 8g --executor-memory 8g\n",
    "```\n",
    "\n",
    "At this point, we've got several sets of data processed and cleansed. We also have discovered several fields we can use for joining:\n",
    "\n",
    "- license_id\n",
    "- longitude, latitude\n",
    "\n",
    "Longitude and latitude are great candidates for joining crime, sanitation, weather, and inspections. The problem is that it's not reasonable to expect them to fall on exactly the same coordinate.\n",
    "\n",
    "Suppose we divided the city up into a grid and determined the coordinates for the center of each cell. Then, we could determine which sanitation complaints and crimes were committed in the cell, and connect that to inspections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import count, datediff, lag, sum, coalesce, rank, lit, when,col, udf, to_date, year, mean, month, date_format, array\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the City Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a grid by finding the boundaries of our coordinates (using crime data because it's the largest set), then assign a grid identifier (`city_grid`). Then, we'll add that id to all of our sets. We'll follow the logical Cassandra pattern of creating a table representing the query we'll want (with the grid identifier as the key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  cartesian\n",
    "# A function that creates the cartesian product/combination\n",
    "# Input: \n",
    "#      x1    (numpy vector 1)\n",
    "#      x2    (numpy vector 2)\n",
    "# Returns: \n",
    "#      cartesian combination\n",
    "def cartesian(x1, x2):\n",
    "    return np.transpose([np.tile(x1, len(x2)), np.repeat(x2, len(x1))])\n",
    "\n",
    "#--------  cartesian\n",
    "# A function that creates \"risk cells\" out of the longitude/latitude combination. That means in \n",
    "# seperates a x, y plane into n cells.\n",
    "# Input: \n",
    "#      longitude\n",
    "#      latitude\n",
    "#      n_cells\n",
    "# Returns: \n",
    "#      risk cells\n",
    "def create_risk_cells(longitude, latitude, n_cells, ward, district):\n",
    "    n = int(np.sqrt(n_cells))\n",
    "    x1 = np.zeros(n)\n",
    "    x2 = np.zeros(n)\n",
    "\n",
    "    min_long = min(longitude)\n",
    "    min_lat = min(latitude)\n",
    "    step_long = (max(longitude) - min(longitude)) / n\n",
    "    step_lat = (max(latitude) - min(latitude)) / n\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        x1[i] = min_long + (step_long * i)\n",
    "        x2[i] = min_lat + (step_lat * i) \n",
    "        \n",
    "    df = pd.DataFrame(cartesian(x1, x2))\n",
    "    df[\"ward\"] = ward\n",
    "    df[\"district\"] = district\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the crime data and map it to risk cells to create a master table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = sqlContext.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "               .load(keyspace=\"chicago_data\", table=\"crime\")\\\n",
    "               .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got some missing districts and wards. Let's convert that to 0 to indicate we don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'40.0', u'27.0', u'23.0', u'25.0', u'42.0', u'19.0', u'5.0',\n",
       "       u'34.0', u'24.0', u'9.0', u'43.0', u'1.0', u'11.0', u'21.0',\n",
       "       u'20.0', u'33.0', u'28.0', u'22.0', u'47.0', u'44.0', u'2.0',\n",
       "       u'15.0', u'30.0', u'46.0', u'36.0', u'8.0', u'38.0', u'6.0',\n",
       "       u'26.0', u'49.0', u'3.0', u'45.0', u'4.0', u'16.0', u'17.0',\n",
       "       u'39.0', u'7.0', u'50.0', u'18.0', u'32.0', u'12.0', u'14.0',\n",
       "       u'37.0', u'35.0', u'29.0', u'41.0', u'10.0', u'13.0', u'48.0',\n",
       "       u'31.0', u'NaN'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.ward.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'20.0', u'11.0', u'8.0', u'12.0', u'18.0', u'22.0', u'3.0', u'5.0',\n",
       "       u'9.0', u'6.0', u'10.0', u'17.0', u'19.0', u'1.0', u'7.0', u'25.0',\n",
       "       u'16.0', u'24.0', u'2.0', u'4.0', u'14.0', u'15.0', u'31.0', u'NaN',\n",
       "       u'23.0'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.district.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test['ward'] = pd.to_numeric(df_test.ward, errors='coerce').fillna(0).astype(int)\n",
    "df_test['district'] = pd.to_numeric(df_test.district, errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>ward</th>\n",
       "      <th>district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-91.686569</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-91.644949</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-91.603328</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-91.561708</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-91.520088</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1  ward  district\n",
       "0 -91.686569  36.619446    40        20\n",
       "1 -91.644949  36.619446    27        11\n",
       "2 -91.603328  36.619446    23         8\n",
       "3 -91.561708  36.619446    25        12\n",
       "4 -91.520088  36.619446    42        18"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid = create_risk_cells(df_test.longitude, df_test.latitude, 100*100, df_test.ward, df_test.district)\n",
    "df_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center_longitude</th>\n",
       "      <th>center_latitude</th>\n",
       "      <th>ward</th>\n",
       "      <th>police_district</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-91.686569</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-91.644949</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-91.603328</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-91.561708</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-91.520088</td>\n",
       "      <td>36.619446</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   center_longitude  center_latitude  ward  police_district  id\n",
       "0        -91.686569        36.619446    40               20   0\n",
       "1        -91.644949        36.619446    27               11   1\n",
       "2        -91.603328        36.619446    23                8   2\n",
       "3        -91.561708        36.619446    25               12   3\n",
       "4        -91.520088        36.619446    42               18   4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid.columns=[\"center_longitude\", \"center_latitude\", \"ward\", \"police_district\"]\n",
    "df_grid[\"id\"] = df_grid.index\n",
    "df_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll save that master table. It'll only be useful for human readability. We won't use it in analysis from here on out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cql\n",
    "CREATE TABLE chicago_data.city_grid (\n",
    "    id int,\n",
    "    center_latitude float,\n",
    "    center_longitude float,\n",
    "    ward int,\n",
    "    police_district int,\n",
    "    PRIMARY KEY (id));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the grid cells\n",
    "sqlContext.createDataFrame(df_grid).write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"city_grid\", keyspace=\"chicago_data\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use KNN to find out which grid cell each of our inspections are in. If you're not familiar with it, KNN is an ML classification algorithm that calculates the distance between (let's say the Euclidian distance) an item in question, and it's $k$ nearest neighbors. Mathematically, for $k=1$ it looks like this:\n",
    "\n",
    "$$\\hat{y} = \\min \\sqrt{\\sum_{i=1}^{k} (x_i-y_i)^2}$$\n",
    "\n",
    "Because of that, it's computationally intensive, so it requires the entire set to be in memory and traversed each time. That's ok because we'll do this during cleanup and save the results to a table to use when we run the various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_inspections = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"chicago_data\", table=\"inspections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use knn to figure out which cell you're in. Unfortunately, MLlib doesn't seem to have KNN for classification. \n",
    "#We could use this: https://github.com/saurfang/spark-knn or we could just use sklearn, since we're already in a pandas\n",
    "#dataframe\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "\n",
    "#this gives us something for the model to predict. It doesn't matter that they are all labels.\n",
    "knn = KNN(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, we've got 117 inspections without the coordinates entered. We need to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections.filter(col(\"longitude\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the license records have the GPS coordinates entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_license_coords = sqlContext.sql(\"select license_id, latitude as lat, longitude as long from chicago_data.licenses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(license_id=u'2387978', lat=41.86563491821289, long=-87.7695541381836)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_license_coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_inspections_joined = df_inspections.join(df_license_coords, on=\"license_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['license_id',\n",
       " 'inspection_dt',\n",
       " 'canvass',\n",
       " 'complaint',\n",
       " 'cumulative_failures',\n",
       " 'cumulative_inspections',\n",
       " 'days_since_last_inspection',\n",
       " 'ever_failed',\n",
       " 'fire',\n",
       " 'inspection_date_string',\n",
       " 'inspection_type',\n",
       " 'inspection_type_description',\n",
       " 'latitude',\n",
       " 'license_related',\n",
       " 'liquor',\n",
       " 'longitude',\n",
       " 'month',\n",
       " 'prev_fail',\n",
       " 'proportion_past_failures',\n",
       " 'recent_inspection',\n",
       " 'reinspection',\n",
       " 'risk',\n",
       " 'risk_description',\n",
       " 'special_event',\n",
       " 'task_force',\n",
       " 'weekday',\n",
       " 'weekday_description',\n",
       " 'y',\n",
       " 'y_description',\n",
       " 'y_fail',\n",
       " 'zip',\n",
       " 'lat',\n",
       " 'long']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections_joined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_inspections2 = df_inspections_joined.select('license_id', 'inspection_dt', 'canvass', 'complaint', 'cumulative_failures', \\\n",
    " 'cumulative_inspections', 'days_since_last_inspection', 'ever_failed', 'fire', 'inspection_date_string', 'inspection_type', \\\n",
    " 'inspection_type_description', 'license_related', 'liquor', 'month', \\\n",
    " 'prev_fail', 'proportion_past_failures', 'recent_inspection', 'reinspection', 'risk', 'risk_description', \\\n",
    " 'special_event', 'task_force', 'weekday', 'weekday_description', 'y', 'y_description', 'y_fail', 'zip', \\\n",
    " coalesce(df_inspections_joined[\"latitude\"], df_inspections_joined[\"lat\"]).alias(\"latitude\"),\n",
    " coalesce(df_inspections_joined[\"longitude\"], df_inspections_joined[\"long\"]).alias(\"longitude\"),                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections2.filter(col(\"longitude\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... that did not help at all. Such is the life of a data scientist. ok! `coalesce` to fill in our missing data from the license set was worth a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_inspections2 = df_inspections2.filter(col(\"longitude\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a similar issue with `days_since_last_inspection` since the first year in the set has no previous inspections. We'll just set that to zero to avoid nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_inspections2 = df_inspections2.withColumn(\"days_since_last_inspection\", coalesce(col(\"days_since_last_inspection\"), lit(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll compute the `city_grid` for each license_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = pd.Series(range(0, df_grid.shape[0], 1))\n",
    "fit_knn = knn.fit(df_grid[[\"center_longitude\", \"center_latitude\"]].values, y_train.values)\n",
    "x_test = df_inspections2.toPandas()[[\"longitude\", \"latitude\", \"license_id\"]].values\n",
    "\n",
    "inspections_gridspots = pd.DataFrame(fit_knn.predict(x_test[:,0:2])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78122"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inspections_gridspots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the computed grids with the `license_id` and coordinates so that we can use that to join them to our  inspections on `license_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[u'-87.76052657', u'41.93145721', u'2427620', 9894.0],\n",
       "       [u'-87.56487511', u'41.76465891', u'2093694', 9599.0],\n",
       "       [u'-87.56487511', u'41.76465891', u'2093694', 9599.0],\n",
       "       ..., \n",
       "       [u'-87.62612449', u'41.87362569', u'2060210', 9798.0],\n",
       "       [u'-87.62612449', u'41.87362569', u'2060210', 9798.0],\n",
       "       [u'-87.62612449', u'41.87362569', u'2060210', 9798.0]], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((x_test,inspections_gridspots), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>license_id</th>\n",
       "      <th>city_grid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-87.76052657</td>\n",
       "      <td>41.93145721</td>\n",
       "      <td>2427620</td>\n",
       "      <td>9894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-87.56487511</td>\n",
       "      <td>41.76465891</td>\n",
       "      <td>2093694</td>\n",
       "      <td>9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-87.56487511</td>\n",
       "      <td>41.76465891</td>\n",
       "      <td>2093694</td>\n",
       "      <td>9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-87.56487511</td>\n",
       "      <td>41.76465891</td>\n",
       "      <td>2093694</td>\n",
       "      <td>9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-87.63405907</td>\n",
       "      <td>41.85410541</td>\n",
       "      <td>1998584</td>\n",
       "      <td>9797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      longitude     latitude license_id city_grid\n",
       "0  -87.76052657  41.93145721    2427620      9894\n",
       "1  -87.56487511  41.76465891    2093694      9599\n",
       "2  -87.56487511  41.76465891    2093694      9599\n",
       "3  -87.56487511  41.76465891    2093694      9599\n",
       "4  -87.63405907  41.85410541    1998584      9797"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections3 = pd.DataFrame(np.concatenate((x_test,inspections_gridspots), axis=1))\n",
    "df_inspections3.columns=[\"longitude\", \"latitude\", \"license_id\", \"city_grid\"]\n",
    "df_inspections3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_inspections4 = df_inspections3.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_inspections4 = pd.merge(df_inspections4, df_grid, left_on=\"city_grid\", right_on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the join, we need to take this pandas dataframe and convert it to a Spark dataframe. That gives us the added benefit of being able to drop it back to Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_inspections4 = sqlContext.createDataFrame(df_inspections4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_inspections5 = df_inspections2.join(df_inspections4.select(\"license_id\", \"city_grid\", \"ward\", \"police_district\"), on=\"license_id\", how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(license_id=u'1042702', inspection_dt=datetime.date(2016, 8, 26), canvass=1, complaint=0, cumulative_failures=2, cumulative_inspections=9, days_since_last_inspection=-8, ever_failed=1, fire=0, inspection_date_string=u'8/26/16', inspection_type=3, inspection_type_description=u'Canvass Re-Inspection', license_related=0, liquor=0, month=8, prev_fail=1, proportion_past_failures=0.2222222222222222, recent_inspection=0, reinspection=1, risk=1, risk_description=u'Risk 1 (High)', special_event=0, task_force=0, weekday=2, weekday_description=u'Fri', y=1, y_description=u'Pass', y_fail=0, zip=u'60614', latitude=u'41.93041201', longitude=u'-87.64388653', city_grid=9897.0, ward=4, police_district=2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78274"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[license_id: string, inspection_dt: date, canvass: int, complaint: int, cumulative_failures: int, cumulative_inspections: int, days_since_last_inspection: int, ever_failed: int, fire: int, inspection_date_string: string, inspection_type: int, inspection_type_description: string, license_related: int, liquor: int, month: int, prev_fail: int, proportion_past_failures: double, recent_inspection: int, reinspection: int, risk: int, risk_description: string, special_event: int, task_force: int, weekday: int, weekday_description: string, y: int, y_description: string, y_fail: int, zip: string, latitude: string, longitude: string, city_grid: double, ward: bigint, police_district: bigint]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections5.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('license_id', 'string'),\n",
       " ('inspection_dt', 'date'),\n",
       " ('canvass', 'int'),\n",
       " ('complaint', 'int'),\n",
       " ('cumulative_failures', 'int'),\n",
       " ('cumulative_inspections', 'int'),\n",
       " ('days_since_last_inspection', 'int'),\n",
       " ('ever_failed', 'int'),\n",
       " ('fire', 'int'),\n",
       " ('inspection_date_string', 'string'),\n",
       " ('inspection_type', 'int'),\n",
       " ('inspection_type_description', 'string'),\n",
       " ('license_related', 'int'),\n",
       " ('liquor', 'int'),\n",
       " ('month', 'int'),\n",
       " ('prev_fail', 'int'),\n",
       " ('proportion_past_failures', 'double'),\n",
       " ('recent_inspection', 'int'),\n",
       " ('reinspection', 'int'),\n",
       " ('risk', 'int'),\n",
       " ('risk_description', 'string'),\n",
       " ('special_event', 'int'),\n",
       " ('task_force', 'int'),\n",
       " ('weekday', 'int'),\n",
       " ('weekday_description', 'string'),\n",
       " ('y', 'int'),\n",
       " ('y_description', 'string'),\n",
       " ('y_fail', 'int'),\n",
       " ('zip', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('city_grid', 'double'),\n",
       " ('ward', 'bigint'),\n",
       " ('police_district', 'bigint')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inspections5.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our new table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cql\n",
    "CREATE TABLE chicago_data.inspections_by_city_grid (\n",
    "    city_grid int,\n",
    "    license_id text,\n",
    "    risk_description text,\n",
    "    zip text,\n",
    "    inspection_date_string text,\n",
    "    inspection_type_description text,\n",
    "    y_description text,\n",
    "    latitude text,\n",
    "    longitude text,\n",
    "    y int,\n",
    "    y_fail int,\n",
    "    reinspection int,\n",
    "    recent_inspection int,\n",
    "    task_force int,\n",
    "    special_event int,\n",
    "    canvass int,\n",
    "    fire int,\n",
    "    liquor int,\n",
    "    complaint int,\n",
    "    license_related int,\n",
    "    inspection_type int,\n",
    "    risk int,\n",
    "    inspection_dt date,\n",
    "    prev_fail int,\n",
    "    cumulative_failures int,\n",
    "    weekday_description text,\n",
    "    month int,\n",
    "    weekday int,\n",
    "    ever_failed int,\n",
    "    cumulative_inspections int,\n",
    "    proportion_past_failures double,\n",
    "    days_since_last_inspection int,\n",
    "    ward int,\n",
    "    police_district int,\n",
    "    PRIMARY KEY (city_grid, license_id, inspection_dt));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_inspections5.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"inspections_by_city_grid\", keyspace=\"chicago_data\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage is that we can now push back some aggregation to cassandra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same thing for crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_crime = sqlContext.sql(\"select * from chicago_data.crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crime.filter(col(\"longitude\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude', 'latitude', 'id']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crime.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to retrain the model. The grid spots are correct. We'll use that model to \"predict\" these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_test = df_crime.toPandas()[[\"longitude\", \"latitude\", \"id\"]].values\n",
    "x_test = df_crime.toPandas().values\n",
    "crime_gridspots = pd.DataFrame(fit_knn.predict(x_test[:,0:2])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1822721"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crime.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1822721, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_gridspots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime2 = pd.DataFrame(np.concatenate((x_test,crime_gridspots), axis=1))\n",
    "df_crime2.columns=[\"longitude\", \"latitude\", \"id\", \"city_grid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime2 = df_crime2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime2 = sqlContext.createDataFrame(df_crime2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime_final = df_crime.join(df_crime2.select(\"id\", \"city_grid\"), on=\"id\", how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After joining, we'll add this data to our new table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cql\n",
    "CREATE TABLE chicago_data.crime_by_city_grid (\n",
    "    city_grid int,\n",
    "    id text,\n",
    "    case_number text,\n",
    "    date text,\n",
    "    block text,\n",
    "    iucr text,\n",
    "    primary_type text,\n",
    "    arrest boolean,\n",
    "    beat text,\n",
    "    district text,\n",
    "    ward text,\n",
    "    community_area text,\n",
    "    fbi_code text,\n",
    "    year text,\n",
    "    latitude float,\n",
    "    longitude float,\n",
    "    PRIMARY KEY (city_grid, id));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1822721"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crime_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crime_final.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"crime_by_city_grid\", keyspace=\"chicago_data\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to store the data by type of crime (so that we can use that in aggregation later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cql\n",
    "CREATE TABLE chicago_data.crime_by_type (\n",
    "    primary_type text,\n",
    "    city_grid int,\n",
    "    id int,\n",
    "    PRIMARY KEY (primary_type, city_grid, id));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_crime_final.select(\"id\", \"city_grid\", \"primary_type\").write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"crime_by_type\", keyspace=\"chicago_data\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanitation data is the same, again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sanitation = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"chicago_data\", table=\"sanitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sanitation.filter(col(\"longitude\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['service_request_number',\n",
       " 'community_area',\n",
       " 'completion_date',\n",
       " 'creation_date',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'police_district',\n",
       " 'status',\n",
       " 'street_address',\n",
       " 'type_of_service_request',\n",
       " 'ward',\n",
       " 'what_is_the_nature_of_this_code_violation?',\n",
       " 'zip_code']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sanitation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = df_sanitation.toPandas()[[\"longitude\", \"latitude\", \"service_request_number\"]].values\n",
    "sanitation_gridspots = pd.DataFrame(fit_knn.predict(x_test[:,0:2])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112086"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sanitation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sanitation2 = pd.DataFrame(np.concatenate((x_test,sanitation_gridspots), axis=1))\n",
    "df_sanitation2.columns=[\"longitude\", \"latitude\", \"service_request_number\", \"city_grid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sanitation2 = sqlContext.createDataFrame(df_sanitation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sanitation_final = df_sanitation.join(df_sanitation2.select(\"service_request_number\", \"city_grid\"), on=\"service_request_number\", how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['service_request_number',\n",
       " 'community_area',\n",
       " 'completion_date',\n",
       " 'creation_date',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'police_district',\n",
       " 'status',\n",
       " 'street_address',\n",
       " 'type_of_service_request',\n",
       " 'ward',\n",
       " 'what_is_the_nature_of_this_code_violation?',\n",
       " 'zip_code',\n",
       " 'city_grid']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sanitation_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cql\n",
    "CREATE TABLE chicago_data.sanitation_by_city_grid (\n",
    "    city_grid int,\n",
    "    creation_date text,\n",
    "    status text,\n",
    "    completion_date text,\n",
    "    service_request_number text,\n",
    "    type_of_service_request text,\n",
    "    \"what_is_the_nature_of_this_code_violation?\" text,\n",
    "    street_address text,\n",
    "    zip_code text,\n",
    "    ward text,\n",
    "    police_district double,\n",
    "    community_area double,\n",
    "    latitude double,\n",
    "    longitude double,\n",
    "    PRIMARY KEY (city_grid, service_request_number));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sanitation_final.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"sanitation_by_city_grid\", keyspace=\"chicago_data\")\\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
